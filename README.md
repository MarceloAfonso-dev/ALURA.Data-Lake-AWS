# üèóÔ∏è Data Lake na AWS ‚Äì Projeto de Estudos

<div align="center">

| Status | Forma√ß√£o | Instrutora |
| ------ | -------- | ---------- |
| üöß Em andamento | **AWS Data Lake** (Alura) | **Ana Hashimoto**

</div>

Este reposit√≥rio **n√£o** √© um guia para clonar e executar uma infraestrutura localmente.  
Ele √© uma **vitrine** do que aprendi ao longo da forma√ß√£o, reunindo trechos de c√≥digo Python (sem credenciais!), prints do console AWS e explica√ß√µes conceituais.

---

## üìö Conte√∫do

1. [Vis√£o Geral](#vis√£o-geral)  
2. [Arquitetura & Servi√ßos](#arquitetura--servi√ßos)  
3. [Pipeline de Dados ‚Äì Camadas](#pipeline-de-dados--camadas)  
4. [Passo-a-Passo](#passo-a-passo)  
   1. [Ingest√£o](#11-ingest√£o)  
   2. [Processamento & ETL](#12-processamento--etl)  
   3. [Governan√ßa & Cat√°logo](#13-governan√ßa--cat√°logo)  
   4. [Insights & Dashboard](#14-insights--dashboard) *(pr√≥ximos passos)*  
5. [Trechos de C√≥digo Python](#trechos-de-c√≥digo-python)  
6. [Boas Pr√°ticas & Aprendizados](#boas-pr√°ticas--aprendizados)  
7. [Roadmap Pessoal](#roadmap-pessoal)  
8. [Cr√©ditos](#cr√©ditos)

---

## Vis√£o Geral

> Constru√≠ uma **pipeline serverless** na AWS que ingere, processa e disponibiliza dados em formato otimizado (Parquet).  
> O projeto segue o padr√£o *Data Lake* (camadas bronze ‚Üí silver ‚Üí gold) e emprega **Infraestrutura como C√≥digo** (IaC) para facilitar reprodutibilidade.

<p align="center">
  <img src="images/architecture.png" alt="Arquitetura Data Lake AWS" width="80%">
</p>

---

## Arquitetura & Servi√ßos

| Camada | Servi√ßo principal | Prop√≥sito |
| ------ | ---------------- | --------- |
| **Ingest√£o** | Amazon S3 + Python (`boto3`) | Armazenar dados previamente convertidos (CSV >> Parquet) na *bronze* |
| **ETL** | AWS Glue (Crawler, Jobs, Data Brew) | Catalogar, limpar e converter para Parquet na *silver* |
| **Processamento distribu√≠do** | AWS EMR + Apache Spark | Agrega√ß√µes pesadas / transforma√ß√£o para a *gold* |
| **Consulta** | Amazon Athena | SQL serverless sobre S3 |
| **Visualiza√ß√£o** | AWS QuickSight | Dashboards interativos |
| **Governan√ßa** | AWS Lake Formation + IAM | Seguran√ßa, RBAC e monitoramento |
| **IaC** | AWS CloudFormation / CDK | Provisionamento repet√≠vel |

---

## Pipeline de Dados & Camadas

1. **Bronze** ‚Äì Dados brutos, sem qualquer transforma√ß√£o.  
2. **Silver** ‚Äì Dados limpos, normalizados e particionados.  
3. **Gold** ‚Äì Agrega√ß√µes e fatos otimizados para BI/Analytics.

---

## Passo-a-Passo

### 1. Ingest√£o

---

#### Cria√ß√£o do Bucket S3  
Defini a pol√≠tica de versionamento e o prefixo `bronze/`
<p align="center">
  <img src="images/aws_pipeline_3.png" alt="Configura√ß√£o do bucket" width="75%">
</p>

#### Coleta de dados externos
Python faz `HTTP GET`, salva localmente, converte em parquet e envia ao S3 via `boto3`
<p align="center">
  <img src="images/aws_pipeline_4.png" alt="Upload parquet Boston" width="75%">
</p>

#### Monitoramento de Custos  
Alerta de gastos + MFA para evitar surpresas

<p align="center">
  <img src="images/aws_pipeline_5.png" alt="Alerta de or√ßamento" width="75%">
</p>

---

### 2. Processamento & ETL

#### Glue Crawler  
Descobre esquema dos CSVs e cria tabelas no Glue Data Catalog

<p align="center">
  <img src="images/aws_pipeline_6.png" alt="Crawler Glue" width="75%">
</p>

#### Glue Job ETL  
Tratamento de dados, cria particionamento por ano, m√™s e envia ao `silver/`

<p align="center">
  <img src="images/aws_pipeline_7.png" alt="Job Glue ETL" width="75%">
</p>

#### Data Quality
Aplica qualidade de dados de maneira simplificada de acordo com os pilares do DAMA

<p align="center">
  <img src="images/aws_pipeline_8.png" alt="Regras Data Quality" width="75%">
</p>

#### Glue DataBrew
Tratamento de dados similar ao PowerBI, sem c√≥digos e totalmente interativo.

<p align="center">
  <img src="images/aws_pipeline_9.png" alt="Painel DataBrew" width="75%">
</p>

---

### 3. Governan√ßa & Cat√°logo

#### Lake Formation  
Define administradores e Data Lake Location

<p align="center">
  <img src="images/aws_pipeline_9.png" alt="Configura√ß√£o Lake Formation" width="75%">
</p>

#### Permiss√µes Granulares  
Controle de acesso a colunas sens√≠veis via LF-Tags e pol√≠ticas IAM.

<p align="center">
  <img src="images/aws_pipeline_10.png" alt="Permiss√µes detalhadas" width="75%">
</p>


### 4. Insights & Dashboard *(pr√≥ximos passos)*

| Planejado | Descri√ß√£o |
| --------- | --------- |
| **EMR** | Completar o curso ‚ÄúAWS Data Lake: processando dados com EMR‚Äù |
| **Athena** | Consultar camadas silver & gold via SQL |
| **QuickSight** | Criar dashboard com storytelling e explora√ß√£o guiada |

---

## Trechos de C√≥digo Python

> **‚ö†Ô∏è As credenciais foram removidas e substitu√≠das por vari√°veis de ambiente.**  
> Siga as [pr√°ticas recomendadas de seguran√ßa](https://docs.aws.amazon.com/pt_br/sdkref/latest/guide/creds-config-files.html).

```python
# notebook/Pipeline_AWS.ipynb 

!mkdir -p data

import os
import requests
import pandas as pd
import boto3
from io import BytesIO

# -------- 1) EXTRA√á√ÉO --------
def extract_data(url: str, local_file: str) -> None:
    """Faz o download do CSV e salva localmente."""
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/91.0.4472.124 Safari/537.36"
        )
    }
    resp = requests.get(url, headers=headers, timeout=60)
    resp.raise_for_status()

    os.makedirs(os.path.dirname(local_file), exist_ok=True)
    with open(local_file, "wb") as f:
        f.write(resp.content)

urls = [
    ("https://data.boston.gov/.../tmp...2015.csv", "data/dados_2015.csv"),
    ("https://data.boston.gov/.../tmp...2016.csv", "data/dados_2016.csv"),
    ("https://data.boston.gov/.../tmp...2017.csv", "data/dados_2017.csv"),
    ("https://data.boston.gov/.../tmp...2018.csv", "data/dados_2018.csv"),
    ("https://data.boston.gov/.../tmp...2019.csv", "data/dados_2019.csv"),
    ("https://data.boston.gov/.../tmp...2020.csv", "data/dados_2020.csv"),
]

for url, path in urls:
    extract_data(url, path)

# -------- 2) LEITURA --------
arquivos = [f"data/dados_{ano}.csv" for ano in range(2015, 2021)]
dfs = {path.split("_")[-1].split(".")[0]: pd.read_csv(path) for path in arquivos}

# -------- 3) ENVIO AO S3 --------
# Credenciais lidas de vari√°veis de ambiente ou perfil AWS CLI
session = boto3.Session(
    region_name="us-east-1",
    # profile_name="default",  # ‚Üê ou defina o profile que preferir
)
s3 = session.client("s3")

BUCKET = "alura-datalakeaws-mac"

# arquivo ‚Äúhello world‚Äù (bronze de teste)
with open("hello-s3.txt", "w") as f:
    f.write("Ol√°, S3")
s3.upload_file("hello-s3.txt", BUCKET, "bronze/hello-s3.txt")

# dados reais em Parquet
for ano, df in dfs.items():
    buf = BytesIO()
    df.to_parquet(buf, index=False)
    buf.seek(0)

    s3.put_object(
        Bucket=BUCKET,
        Key=f"bronze/dados_{ano}.parquet",
        Body=buf.getvalue(),
    )

print("Upload conclu√≠do!")
print("Objetos no bucket:", [obj["Key"] for obj in s3.list_objects(Bucket=BUCKET)["Contents"]])

```

---

## Principais boas pr√°ticas

- **Convers√£o _in-memory_** ‚Üí menos I/O em disco e menos espa√ßo tempor√°rio  
- **Parquet columnar** ‚Üí leitura seletiva em Athena  
- **Session do boto3** ‚Üí nada de chaves hard-coded no c√≥digo

---

## Boas Pr√°ticas & Aprendizados

- **Parquet & Columnar** ‚Äì diminui custo de varredura  
- **Observabilidade** ‚Äì CloudWatch Logs + m√©tricas Glue  
- **Seguran√ßa** ‚Äì princ√≠pio do menor privil√©gio (IAM & Lake Formation)  
- **Custos** ‚Äì alertas de or√ßamento, *lifecycle rules* para arquivamento  
- **DataViz** ‚Äì t√©cnicas do livro *Storytelling com Dados* aplicadas no QuickSight  

---

## Roadmap Pessoal

- [ ] Forma√ß√£o Alura: Data Lake com Pipelines na AWS 
- [ ] Forma√ß√£o Alura: Engenharia de Analytics na AWS
- [ ] Certifica√ß√£o **AWS Data Engineer ‚Äì Associate** at√© **dez / 2025**

---

## Cr√©ditos

Forma√ß√£o **AWS Data Lake** ‚Äì [Alura](https://www.alura.com.br/)  
Instrutora: **Ana Hashimoto**  

Este README √© uma s√≠ntese autoral que **n√£o** distribui material propriet√°rio do curso.

<div align="center">

> ‚ÄúData que fica em p√¢ntano n√£o agrega valor ‚Äì  
> √© a engenharia que transforma o lago em insights.‚Äù

</div>

