# üèóÔ∏è Data Lake na AWS ‚Äì Projeto de Estudos

<div align="center">

| Status | Forma√ß√£o | Instrutora |
| ------ | -------- | ---------- |
| ‚úÖ Conclu√≠do | **AWS Data Lake** (Alura) | **Ana Hashimoto** |

</div>

Este reposit√≥rio **n√£o** √© um guia para clonar e executar uma infraestrutura localmente.  
Ele √© uma **vitrine** do que aprendi ao longo da forma√ß√£o, reunindo trechos de c√≥digo Python (sem credenciais!), prints do console AWS e explica√ß√µes conceituais.

---

## üìö Conte√∫do
1. [Vis√£o Geral](#vis√£o-geral)  
2. [Arquitetura & Servi√ßos](#arquitetura--servi√ßos)  
3. [Pipeline de Dados ‚Äì Camadas](#pipeline-de-dados--camadas)  
4. [Passo-a-Passo](#passo-a-passo)  
   1. [Ingest√£o](#1-ingest√£o)  
   2. [Processamento & ETL](#2-processamento--etl)  
   3. [Governan√ßa & Cat√°logo](#3-governan√ßa--cat√°logo)  
   4. [Processamento com EMR + Spark](#4-processamento-com-emr--spark)
   5. [Consultas com AWS Athena](#5-consultas-com-aws-athena)
   6. [Insights & Dashboard](#6-insights--dashboard)  
5. [Trechos de C√≥digo Python](#trechos-de-c√≥digo-python)  
6. [Boas Pr√°ticas & Aprendizados](#boas-pr√°ticas--aprendizados)  
7. [Roadmap Pessoal](#roadmap-pessoal)  
8. [Cr√©ditos](#cr√©ditos)

---

## Vis√£o Geral

> Constru√≠ uma **pipeline serverless** na AWS que ingere, processa e disponibiliza dados em formato otimizado (Parquet).  
> O projeto segue o padr√£o *Data Lake* (camadas bronze ‚Üí silver ‚Üí gold) para ingest√£o, transforma√ß√£o e an√°lise dos dados.


<p align="center">
  <img src="images/architecture.png" alt="Arquitetura Data Lake AWS" width="80%">
</p>

---

## Arquitetura & Servi√ßos

| Camada | Servi√ßo principal | Prop√≥sito |
| ------ | ---------------- | --------- |
| **Ingest√£o** | Amazon S3 + Python (`boto3`) | Armazenar dados previamente convertidos (CSV >> Parquet) na *bronze* |
| **ETL** | AWS Glue (Crawler, Jobs, Data Brew) | Catalogar, limpar e converter para Parquet na *silver* |
| **Processamento distribu√≠do** | AWS EMR + Apache Spark | Agrega√ß√µes pesadas / transforma√ß√£o para a *gold* |
| **Consulta** | Amazon Athena | SQL serverless sobre S3 |
| **Visualiza√ß√£o** | AWS QuickSight | Dashboards interativos |
| **Governan√ßa** | AWS Lake Formation + IAM | Seguran√ßa, RBAC e monitoramento |

---

## Pipeline de Dados & Camadas

1. **Bronze** ‚Äì Dados brutos, sem qualquer transforma√ß√£o.  
2. **Silver** ‚Äì Dados limpos, normalizados e particionados.  
3. **Gold** ‚Äì Agrega√ß√µes e fatos otimizados para BI/Analytics.

---

## Passo-a-Passo

### 1. Ingest√£o

---

#### Cria√ß√£o do Bucket S3  
Defini a pol√≠tica de versionamento e o prefixo `bronze/`
<p align="center">
  <img src="images/aws_pipeline_3.png" alt="Configura√ß√£o do bucket" width="75%">
</p>

#### Coleta de dados externos
Python faz `HTTP GET`, salva localmente, converte em parquet e envia ao S3 via `boto3`
<p align="center">
  <img src="images/aws_pipeline_4.png" alt="Upload parquet Boston" width="75%">
</p>

#### Monitoramento de Custos  
Alerta de gastos + MFA para evitar surpresas

<p align="center">
  <img src="images/aws_pipeline_5.png" alt="Alerta de or√ßamento" width="75%">
</p>

---

### 2. Processamento & ETL

#### Glue Crawler  
Descobre esquema dos CSVs e cria tabelas no Glue Data Catalog

<p align="center">
  <img src="images/aws_pipeline_6.png" alt="Crawler Glue" width="75%">
</p>

#### Glue Job ETL  
Tratamento de dados, cria particionamento por ano, m√™s e envia ao `silver/`

<p align="center">
  <img src="images/aws_pipeline_7.png" alt="Job Glue ETL" width="75%">
</p>

#### Data Quality
Aplica qualidade de dados de maneira simplificada de acordo com os pilares do DAMA

<p align="center">
  <img src="images/aws_pipeline_8.png" alt="Regras Data Quality" width="75%">
</p>

#### Glue DataBrew
Tratamento de dados similar ao PowerBI, sem c√≥digos e totalmente interativo.

<p align="center">
  <img src="images/aws_pipeline_9.png" alt="Painel DataBrew" width="75%">
</p>

---

### 3. Governan√ßa & Cat√°logo

#### Lake Formation  
Define administradores e Data Lake Location

<p align="center">
  <img src="images/aws_pipeline_10.png" alt="Configura√ß√£o Lake Formation" width="75%">
</p>

#### Permiss√µes Granulares  
Controle de acesso a colunas / linhas sens√≠veis via Lake Formation Data Filters.

<p align="center">
  <img src="images/aws_pipeline_11.png" alt="Permiss√µes detalhadas" width="75%">
</p>

---

### 4. Processamento com EMR + Spark

- Cria√ß√£o de cluster **AWS EMR** para processar dados do S3.
- Execu√ß√£o de **PySpark** para transforma√ß√£o em batch.
- Uso de Spark CLI para jobs automatizados.

<p align="center">
  <img src="images/aws_pipeline_12.png" alt="AWS EMR Cluster" width="75%">
</p>

---
### 5. Consultas com AWS Athena

- **Athena** permite executar SQL diretamente sobre os dados armazenados no S3, sem necessidade de provisionar servidores.
- Cada camada (**bronze**, **silver**, **gold**) pode ser consultada via tabelas criadas no Glue Data Catalog.
- Exemplos de uso:
  - Explorar dados brutos da camada bronze para valida√ß√£o inicial.
  - Realizar queries otimizadas sobre dados limpos e particionados na silver.
  - Gerar relat√≥rios e an√°lises r√°pidas sobre agrega√ß√µes da gold.
- Integra√ß√£o nativa com **Lake Formation** para controle de acesso granular.
- Resultados das queries podem ser exportados para CSV, visualizados no console ou integrados ao QuickSight.

<p align="center">
  <img src="images/aws_pipeline_13.png" alt="Consulta Athena" width="75%">
</p>

--- 

### 6. Insights & Dashboard

- **QuickSight**: cria√ß√£o de an√°lises visuais utilizando datasets no S3 (via Athena).
- Configura√ß√£o de **SPICE** para acelera√ß√£o de queries.
- Boas pr√°ticas de **DataViz** aplicadas:
  - Escolha de gr√°ficos adequados
  - Uso de cores consistentes
  - Storytelling no dashboard
- Uso de **Reader vs Author** para controle de custos.

<p align="center">
  <img src="images/aws_pipeline_14.png" alt="QuickSight Dashboard" width="75%">
</p>
<p align="center">
  <img src="images/aws_pipeline_15.jpg" alt="QuickSight Dashboard" width="75%">
</p>

---


## Trechos de C√≥digo Python de upload bronze

> **‚ö†Ô∏è As credenciais foram removidas e substitu√≠das por vari√°veis de ambiente.**  
> Siga as [pr√°ticas recomendadas de seguran√ßa](https://docs.aws.amazon.com/pt_br/sdkref/latest/guide/creds-config-files.html).

```python
# notebook/Pipeline_AWS.ipynb 

!mkdir -p data

import os
import requests
import pandas as pd
import boto3
from io import BytesIO

# -------- 1) EXTRA√á√ÉO --------
def extract_data(url: str, local_file: str) -> None:
    """Faz o download do CSV e salva localmente."""
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/91.0.4472.124 Safari/537.36"
        )
    }
    resp = requests.get(url, headers=headers, timeout=60)
    resp.raise_for_status()

    os.makedirs(os.path.dirname(local_file), exist_ok=True)
    with open(local_file, "wb") as f:
        f.write(resp.content)

urls = [
    ("https://data.boston.gov/.../tmp...2015.csv", "data/dados_2015.csv"),
    ("https://data.boston.gov/.../tmp...2016.csv", "data/dados_2016.csv"),
    ("https://data.boston.gov/.../tmp...2017.csv", "data/dados_2017.csv"),
    ("https://data.boston.gov/.../tmp...2018.csv", "data/dados_2018.csv"),
    ("https://data.boston.gov/.../tmp...2019.csv", "data/dados_2019.csv"),
    ("https://data.boston.gov/.../tmp...2020.csv", "data/dados_2020.csv"),
]

for url, path in urls:
    extract_data(url, path)

# -------- 2) LEITURA --------
arquivos = [f"data/dados_{ano}.csv" for ano in range(2015, 2021)]
dfs = {path.split("_")[-1].split(".")[0]: pd.read_csv(path) for path in arquivos}

# -------- 3) ENVIO AO S3 --------
# Credenciais lidas de vari√°veis de ambiente ou perfil AWS CLI
session = boto3.Session(
    region_name="us-east-1",
    # profile_name="default",  # ‚Üê ou defina o profile que preferir
)
s3 = session.client("s3")

BUCKET = "alura-datalakeaws-mac"

# arquivo ‚Äúhello world‚Äù (bronze de teste)
with open("hello-s3.txt", "w") as f:
    f.write("Ol√°, S3")
s3.upload_file("hello-s3.txt", BUCKET, "bronze/hello-s3.txt")

# dados reais em Parquet
for ano, df in dfs.items():
    buf = BytesIO()
    df.to_parquet(buf, index=False)
    buf.seek(0)

    s3.put_object(
        Bucket=BUCKET,
        Key=f"bronze/dados_{ano}.parquet",
        Body=buf.getvalue(),
    )

print("Upload conclu√≠do!")
print("Objetos no bucket:", [obj["Key"] for obj in s3.list_objects(Bucket=BUCKET)["Contents"]])

```

## Trechos de C√≥digo Python de processamento com Spark/EMR

> Script utilizado para transformar dados na camada gold usando PySpark no EMR.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, unix_timestamp, when
from pyspark.sql.types import TimestampType
import argparse

def transform_data(database:str, table_source:str, table_target:str) -> None:
    spark = (SparkSession.builder
             .appName("Boston 311 Service Requests Analysis")
             .enableHiveSupport()
             .getOrCreate())
    
    df = spark.read.table(f"`{database}`.`{table_source}`")

    df = (df.withColumn("open_dt", col("open_dt").cast(TimestampType()))
          .withColumn("closed_dt", col("closed_dt").cast(TimestampType()))
          .withColumn("target_dt", col("target_dt").cast(TimestampType()))
    )

    df = df.withColumn("delay_days", when
                      (col("closed_dt")>col("target_dt"),
                      (unix_timestamp(col("closed_dt"))-unix_timestamp(col("target_dt")))/86400, ).otherwise(0), )
    
    columns_to_keep = [
        "case_enquiry_id",
        "open_dt",
        "closed_dt",
        "target_dt",
        "case_status",
        "ontime",
        "closure_reason_normalized",
        "case_title",
        "subject",
        "reason",
        "neighborhood",
        "location_street_name",
        "location_zipcode",
        "latitude",
        "longitude",
        "source",
        "delay_days",
    ]

    df_selected = df.select(columns_to_keep)

    df_selected.createOrReplaceTempView("boston_311_data")

    query = """
    SELECT * FROM boston_311_data
    WHERE case_status = 'Closed'
    AND delay_days > 0
    ORDER BY delay_days DESC
    """

    result_df = spark.sql(query)

    result_df.write.mode("overwrite").format("parquet").insertInto(f"`{database}`.`{table_target}`", overwrite=True)


    spark.stop()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Transformar dados de solicita√ß√µes de servi√ßo de Boston 311"
    )
    parser.add_argument("--database", type=str, help="Nome do banco de dados no Glue Data Catalog")
    parser.add_argument("--table_source", type=str, help="Nome da tabela origem no Glue Data Catalog")
    parser.add_argument("--table_target", type=str, help="Nome da tabela destino no Glue Data Catalog")

    args = parser.parse_args()

    transform_data(args.database, args.table_source, args.table_target)

    
```

---

> üîê **Sobre credenciais**  
> - Todas as chamadas `boto3` utilizam um **usu√°rio IAM de menor privil√©gio** criado especificamente para este projeto (chaves armazenadas em vari√°veis de ambiente).  
> - A **conta root n√£o √© usada** em nenhuma etapa; MFA permanece habilitado.  
> - A pol√≠tica anexada ao usu√°rio cont√©m apenas:  
>   * `s3:PutObject`, `s3:GetObject`, `s3:ListBucket` restritos ao bucket `alura-datalakeaws-mac/*`  
>   * `lakeformation:GetDataAccess` e `athena:*` para consultas controladas pelo Lake Formation  
>   * `glue:*` limitado ao ***Job ETL*** e ao ***Crawler*** deste projeto.  
> - Logs de acesso est√£o em **CloudTrail** ‚Üí S3 e **CloudWatch** para auditoria.

---

## Principais boas pr√°ticas

- **Convers√£o _in-memory_** ‚Üí menos I/O em disco e menos espa√ßo tempor√°rio  
- **Parquet columnar** ‚Üí leitura seletiva em Athena  
- **Session do boto3** ‚Üí nada de chaves hard-coded no c√≥digo

---

## Boas Pr√°ticas & Aprendizados

- **Parquet & Columnar** ‚Äì diminui custo de varredura  
- **Observabilidade** ‚Äì CloudWatch Logs + m√©tricas Glue  
- **Seguran√ßa** ‚Äì princ√≠pio do menor privil√©gio (IAM & Lake Formation)  
- **Custos** ‚Äì alertas de or√ßamento, *lifecycle rules* para arquivamento  
- **DataViz** ‚Äì t√©cnicas do livro *Storytelling com Dados* aplicadas no QuickSight  

---

## Roadmap Pessoal

- [ ] Forma√ß√£o Alura: Data Lake com Pipelines na AWS 
- [ ] Certifica√ß√£o **AWS Data Engineer ‚Äì Associate** at√© **dez / 2025**

---

## Cr√©ditos

Forma√ß√£o **AWS Data Lake** ‚Äì [Alura](https://www.alura.com.br/)  
Instrutora: **Ana Hashimoto**  

Este README √© uma s√≠ntese autoral que **n√£o** distribui material propriet√°rio do curso.

<div align="center">

> ‚ÄúData que fica em p√¢ntano n√£o agrega valor ‚Äì  
> √© a engenharia que transforma o lago em insights.‚Äù

</div>

